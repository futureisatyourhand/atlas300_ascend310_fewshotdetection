graphs {
    graph_id: 21
    device_id: "2"

    engines {
        id: 22
        engine_name: "MindInferenceEngine"
        so_name: "./libmind_inference_engine.so"
	
        ai_config {
            items {
                name: "model_path"
                value: "../../deploy21.om"
            }
            items {
                name: "batch_size"
                value: "32"
            }
        }
        side: DEVICE 
    }

    engines {
        id: 23
        engine_name: "OutputEngine"
        so_name: "./liboutput_engine.so"
        side: HOST
    }

    connects {
        src_engine_id: 22
        target_engine_id: 23
    }

}
